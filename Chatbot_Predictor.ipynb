{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0e2a8e8-c71d-401a-8ab5-3a07c432d7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Requirement already satisfied: pyspark in ./.local/lib/python3.11/site-packages (3.5.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (6.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in ./.local/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b020d45-a9cf-4926-a56b-23473343503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType\n",
    "from pyspark.sql.functions import sum as sum_agg\n",
    "import pyspark.sql.functions as F\n",
    "import yaml\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c731f5f7-4251-4c0c-8dbd-03f8f7e45e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def preprocess_and_tokenize(text):\n",
    "    text = preprocess_text(text)\n",
    "    return tokenize(text)\n",
    "\n",
    "# UDF for preprocessing and tokenizing\n",
    "preprocess_and_tokenize_udf = udf(preprocess_and_tokenize, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a16908c2-a368-4dd6-b1b9-cc39a6d5f521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 16:06:20 WARN Utils: Your hostname, blue-nbjupyterhub5 resolves to a loopback address: 127.0.0.1; using 10.0.0.155 instead (on interface ens5)\n",
      "23/12/18 16:06:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/18 16:06:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"IntentRecognition\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb1b293f-dfc2-43ea-8820-66578cbd374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the YAML file\n",
    "with open('nlu.yml', 'r') as file:\n",
    "    file_content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "177e0527-1211-4637-8e4a-fd4b4d835d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the YAML file\n",
    "parsed_data = yaml.safe_load(file_content)\n",
    "data = [(intent[\"intent\"], example) for intent in parsed_data[\"nlu\"] for example in intent[\"examples\"].split(\"\\n\")[1:] if example.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f399ddc3-c728-4683-b6ba-a8e9e3dd6598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"intent\", \"example\"])\n",
    "\n",
    "# Map Phase: Preprocess and tokenize examples\n",
    "df = df.withColumn(\"tokens\", preprocess_and_tokenize_udf(df[\"example\"]))\n",
    "\n",
    "# Reduce Phase 1: Explode tokens and count occurrences\n",
    "df_token_count = df.withColumn(\"token\", explode(df[\"tokens\"])).groupBy(\"intent\", \"token\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99296fb7-edcb-4d35-a961-dbef5701a4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|              intent|total_score|\n",
      "+--------------------+-----------+\n",
      "|faq/when_was_eCom...|        128|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|        intent|total_score|\n",
      "+--------------+-----------+\n",
      "|search_product|         36|\n",
      "+--------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|              intent|total_score|\n",
      "+--------------------+-----------+\n",
      "|chitchat/ask_lang...|        193|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Test cases\n",
    "test_inputs = [\"when was the company founded?\", \"search ekber\", \"So I'm here Today to ask one very simple question, what are you ?\"]\n",
    "\n",
    "for user_input in test_inputs:\n",
    "    user_tokens = set(preprocess_and_tokenize(user_input))\n",
    "    # Broadcast user input tokens\n",
    "    broadcast_user_input = spark.sparkContext.broadcast(user_tokens)\n",
    "\n",
    "    # Reduce Phase 2: Calculate match score\n",
    "    def match_score(count, token):\n",
    "        if token in broadcast_user_input.value:\n",
    "            return count\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    match_score_udf = udf(match_score, IntegerType())\n",
    "\n",
    "    # Add match score to DataFrame\n",
    "    df_score = df_token_count.withColumn(\"match_score\", match_score_udf(df_token_count[\"count\"], df_token_count[\"token\"]))\n",
    "\n",
    "    # Aggregate scores for each intent\n",
    "    df_intent_score = df_score.groupBy(\"intent\").agg(F.sum(\"match_score\").alias(\"total_score\"))\n",
    "\n",
    "    # Find the intent with the highest match score\n",
    "    result_intent = df_intent_score.orderBy(\"total_score\", ascending=False).limit(1)\n",
    "\n",
    "    result_intent.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097358ef-2c98-442d-86ce-ecb5d379b9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
